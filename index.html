<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Borui Wang</title>
  
  <meta name="author" content="Borui Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/yale_logo.png">
</head>

<body>
  <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:72%;vertical-align:middle">
              <p style="text-align:center">
                <name>Borui Wang</name>
              </p>

              <p>
              Hi, I am a CS Ph.D. Student at the <a href="https://cpsc.yale.edu/">Computer Science Department</a> of <a href="https://www.yale.edu/">Yale University</a>, working on natural language processing and machine learning with <a href="http://www.cs.yale.edu/homes/radev/">Professor Dragomir Radev</a> in the <a href="https://yale-lily.github.io/">LILY</a> (Language, Information, and Learning at Yale) Lab. My research interests are in knowledge reasoning, neuro-symbolic reasoning, logical reasoning, multimodal language grounding, machine reading comprehension, question answering, semantic parsing and pre-trained language models.
              </p>
              
              <p>
              Before coming to Yale, I obtained my Master of Science degree in Computer Science from <a href="https://www.stanford.edu">Stanford University</a>, where I conducted research in machine learning, computer vision and robotics at the <a href="https://ai.stanford.edu">Stanford Artificial Intelligence Laboratory</a>. Prior to that, I spent four wonderful undergraduate years at <a href="https://www.harvard.edu">Harvard University</a> and received my Bachelor of Arts degree in Computer Science with a secondary field in Economics from Harvard.
              </p>

              <p>
              I have also worked in the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> of <a href="https://www.cmu.edu/">Carnegie Mellon University</a> for one year conducting research on machine learning theories. At CMU I work with <a href="http://www.cs.cmu.edu/~ggordon/">Professor Geoff Gordon</a> to design and study new learning algorithms for general latent-variable graphical models.  
              </p>

              <p style="text-align:left">
                <a href="https://scholar.google.com/citations?user=OJyH6gcAAAAJ&hl=en&oi=ao">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/Borui_Wang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Borui_Wang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li><font color="red"><strong>[New]</strong></font> &nbsp; 2022.04.07 - Our paper “KAT: A Knowledge Augmented Transformer for Vision-and-Language” is accepted to NAACL 2022.</li>
                <li><font color="red"><strong>[New]</strong></font> &nbsp; 2022.04.07 - Our paper “CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning” is accepted to NAACL 2022.</li>
                <li><font color="red"><strong>[New]</strong></font> &nbsp; 2022.04.07 - Our paper “Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries” is accepted to NAACL 2022.</li>
                <li><font color="red"><strong>[New]</strong></font> &nbsp; 2021.12.16 - Our <a href="https://arxiv.org/abs/2112.08614">paper</a> “KAT: A Knowledge Augmented Transformer for Vision-and-Language” ranked No. 1 on the OK-VQA challenge <a href="https://okvqa.allenai.org/leaderboard.html">leaderboard</a>. Check it out!</li>
                <li><font color="red"><strong>[New]</strong></font> &nbsp; 2021.05.06 - Our paper “ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining” is accepted to ACL 2021.</li>
                <li>2019.11.10 - Our paper “Learning General Latent-Variable Graphical Models with Predictive Belief Propagation” is accepted to AAAI 2020.</li>
                <li>2019.07.22 - Our paper “Imitation Learning for Human Pose Prediction” is accepted to ICCV 2019.</li>
                <li>2018.11.05 - Our paper “Action-Agnostic Human Pose Forecasting” is accepted to WACV 2019.</li>
              </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          
            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="container">
                  <img src='images/kat.png' width="270" height="auto">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.08614">
                  <papertitle>KAT: A Knowledge Augmented Transformer for Vision-and-Language</papertitle>
                </a>
                <br>
                Liangke Gui, <strong>Borui Wang</strong>, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, Jianfeng Gao
                <br>
                In Proceedings of the 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).
                <br>
                <font style="color: rgb(255,120,0)">NAACL 2022</font>
                <br>
                <a href="https://arxiv.org/abs/2112.08614">[arXiv]</a>
                <br>
                <p></p>
                <p>The primary focus of recent work with largescale transformers has been on optimizing the amount of information packed into the model's parameters. In this work, we ask a different question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a novel model - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6 points absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an end to end encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. An additional benefit of explicit knowledge integration is seen in improved interpretability of model predictions in our analysis.</p>
              </td>
            </tr>

          
            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="container">
                  <img src='images/confit.png' width="270" height="auto">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.08713">
                  <papertitle>CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning</papertitle>
                </a>
                <br>
                Xiangru Tang, Arjun Nair, <strong>Borui Wang</strong>, Bingyao Wang, Jai Desai, Aaron Wade, Haoran Li, Asli Celikyilmaz, Yashar Mehdad, Dragomir Radev
                <br>
                In Proceedings of the 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).
                <br>
                <font style="color: rgb(255,120,0)">NAACL 2022</font>
                <br>
                <a href="https://arxiv.org/abs/2112.08713">[arXiv]</a>
                <br>
                <p></p>
                <p>Factual inconsistencies in generated summaries severely limit the practical applications of abstractive dialogue summarization. Although significant progress has been achieved by using pre-trained models, substantial amounts of hallucinated content are found during the human evaluation. Pre-trained models are most commonly fine-tuned with cross-entropy loss for text summarization, which may not be an optimal strategy. In this work, we provide a typology of factual errors with annotation data to highlight the types of errors and move away from a binary understanding of factuality. We further propose a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called ConFiT. Based on our linguistically-informed typology of errors, we design different modular objectives that each target a specific type. Specifically, we utilize hard negative samples with errors to reduce the generation of factual inconsistency. In order to capture the key information between speakers, we also design a dialogue-specific loss. Using human evaluation and automatic faithfulness metrics, we show that our model significantly reduces all kinds of factual errors on the dialogue summarization, SAMSum corpus. Moreover, our model could be generalized to the meeting summarization, AMI corpus, and it produces significantly higher scores than most of the baselines on both datasets regarding word-overlap metrics.</p>
              </td>
            </tr>
          

            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="container">
                  <img src='images/investigating.png' width="270" height="auto">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2109.09195">
                  <papertitle>Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries</papertitle>
                </a>
                <br>
                Xiangru Tang, Alexander R. Fabbri, Ziming Mao, Griffin Adams, <strong>Borui Wang</strong>, Haoran Li, Yashar Mehdad, Dragomir Radev
                <br>
                In Proceedings of the 2022 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).
                <br>
                <font style="color: rgb(255,120,0)">NAACL 2022</font>
                <br>
                <a href="https://arxiv.org/abs/2109.09195">[arXiv]</a>
                <br>
                <p></p>
                <p>Current pre-trained models applied to summarization are prone to factual inconsistencies which either misrepresent the source text or introduce extraneous information. Thus, comparing the factual consistency of summaries is necessary as we develop improved models. However, the optimal human evaluation setup for factual consistency has not been standardized. To address this issue, we crowdsourced evaluations for factual consistency using the rating-based Likert scale and ranking-based Best-Worst Scaling protocols, on 100 articles from each of the CNN-Daily Mail and XSum datasets over four state-of-the-art models, to determine the most reliable evaluation framework. We find that ranking-based protocols offer a more reliable measure of summary quality across datasets, while the reliability of Likert ratings depends on the target dataset and the evaluation design. Our crowdsourcing templates and summary evaluations will be publicly available to facilitate future research on factual consistency in summarization.</p>
              </td>
            </tr>


            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="container">
                  <img src='images/convosumm.png' width="270" height="auto">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2106.00829">
                  <papertitle>ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining</papertitle>
                </a>
                <br>
                Alexander R. Fabbri, Faiaz Rahman, Imad Rizvi, <strong>Borui Wang</strong>, Haoran Li, Yashar Mehdad, Dragomir Radev
                <br>
                In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021.
                <br>
                <font style="color: rgb(255,120,0)">ACL 2021</font>
                <br>
                <a href="https://arxiv.org/abs/2106.00829">[arXiv]</a>
                <br>
                <p></p>
                <p>Factual inconsistencies in generated summaries severely limit the practical applications of abstractive dialogue summarization. Although significant progress has been achieved by using pre-trained models, substantial amounts of hallucinated content are found during the human evaluation. Pre-trained models are most commonly fine-tuned with cross-entropy loss for text summarization, which may not be an optimal strategy. In this work, we provide a typology of factual errors with annotation data to highlight the types of errors and move away from a binary understanding of factuality. We further propose a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called ConFiT. Based on our linguistically-informed typology of errors, we design different modular objectives that each target a specific type. Specifically, we utilize hard negative samples with errors to reduce the generation of factual inconsistency. In order to capture the key information between speakers, we also design a dialogue-specific loss. Using human evaluation and automatic faithfulness metrics, we show that our model significantly reduces all kinds of factual errors on the dialogue summarization, SAMSum corpus. Moreover, our model could be generalized to the meeting summarization, AMI corpus, and it produces significantly higher scores than most of the baselines on both datasets regarding word-overlap metrics.</p>
              </td>
            </tr>

          
            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="container">
                  <img src='images/future.jpg' width="270" height="270">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1712.02046">
                  <papertitle>Learning General Latent-Variable Graphical Models with Predictive Belief Propagation</papertitle>
                </a>
                <br>
                <strong>Borui Wang</strong>, Geoffrey Gordon
                <br>
                In Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI), 2020.
                <br>
                <font style="color: rgb(255,120,0)">AAAI 2020</font>
                <br>
                <a href="https://arxiv.org/abs/1712.02046">[arXiv]</a>
                <br>
                <p></p>
                <p>We introduce a novel formulation of message-passing inference over junction trees named predictive belief propagation, and propose a new learning and inference algorithm for general latent-variable graphical models based on this formulation. Our proposed algorithm reduces the hard parameter learning problem into a sequence of supervised learning problems, and unifies the learning of different kinds of latent graphical models into a single learning framework that is local-optima-free and statistically consistent.</p>
              </td>
            </tr>


            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="container">
                  <img src='images/GAIL.jpg' width="270" height="110">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1909.03449">
                  <papertitle>Imitation Learning for Human Pose Prediction</papertitle>
                </a>
                <br>
                <strong>Borui Wang</strong>, Ehsan Adeli, Hsu-kuang Chiu, De-An Huang, Juan Carlos Niebles
                <br>
                In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.
                <br>
                <font style="color: rgb(255,120,0)">ICCV 2019</font>
                <br>
                <a href="https://arxiv.org/abs/1909.03449">[arXiv]</a> / 
                <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Imitation_Learning_for_Human_Pose_Prediction_ICCV_2019_paper.pdf">[CVF Open Access]</a>
                <br>
                <p></p>
                <p>We propose a new reinforcement learning formulation for the problem of human pose prediction in videos, and develop an imitation learning algorithm for predicting future poses under this formulation through a combination of behavioral cloning and generative adversarial imitation learning.</p>
              </td>
            </tr>


            <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="container">
                  <img src='images/TP-RNN.png' width="270" height="135">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1810.09676">
                  <papertitle>Action-Agnostic Human Pose Forecasting</papertitle>
                </a>
                <br>
                Hsu-kuang Chiu, Ehsan Adeli, <strong>Borui Wang</strong>, De-An Huang, Juan Carlos Niebles
                <br>
                In Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV), 2019
                <br>
                <font style="color: rgb(255,120,0)">WACV 2019</font>
                <br>
                <a href="https://arxiv.org/abs/1810.09676">[arXiv]</a> 
                <br>
                <p></p>
                <p>We propose a new action-agnostic method for both short-term and long-term human pose forecasting. To this end, we propose a new recurrent neural network for modeling the hierarchical and multi-scale characteristics of the human motion dynamics, denoted by triangular-prism RNN (TP-RNN). Our model captures the latent hierarchical structure embedded in temporal human pose sequences by encoding the temporal dependencies with different time-scales.</p>
              </td>
            </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
              <br>
              <p align="right"><font size="2">
                <a href="https://people.eecs.berkeley.edu/~barron/">Template</a>
                </font>
              </p>
              </td>
            </tr>



        </td>
      </tr>
    </table>
  </body>

  </html>
